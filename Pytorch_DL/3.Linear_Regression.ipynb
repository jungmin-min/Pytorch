{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ff6aa6a",
   "metadata": {},
   "source": [
    "# 3. 선형 회귀(Linear Regression\n",
    "## 3-1. 선형 회귀(Linear Regression)\n",
    "### 1. 이론"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1fb85a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. 데이터에 대한 이해\n",
    "# 훈련 데이터셋과 테스트 데이터셋 필요\n",
    "\n",
    "# 2. 가설(Hypothesis) 수립\n",
    "# 선형 회귀의 가설(=식): y = Wx + b\n",
    "\n",
    "# 3. 비용 함수(Cost function)에 대한 이해\n",
    "# 비용 함수 = 손실 함수(loss function) = 오차 함수(error function) = 목적 함수(objective function)\n",
    "\n",
    "# 4. 옵티마이저 - 경사 하강법(Gradient Descent)\n",
    "# 비용 함수를 최소로 하는 W와 b를 찾는 방법. 머신 러닝에서의 training이라고 부름."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5d51df",
   "metadata": {},
   "source": [
    "### 2. 파이토치로 선형 회귀 구현하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8f3dff45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f4f1c06a690>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. 기본 셋팅\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "# 현재 실습하고 있는 파이썬 코드르 재실행해도 다음에도 같은 결과가 나오도록 랜덤 시드(random seed)를 준다.\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7b340b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "torch.Size([3, 1])\n",
      "tensor([[2.],\n",
      "        [4.],\n",
      "        [6.]])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "# 2. 변수 선언\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.shape)\n",
    "print(y_train)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "19ec096a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.], requires_grad=True)\n",
      "tensor([0.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# 3. 가중치와 편향의 초기화\n",
    "# 가중치 W를 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "print(W)\n",
    "\n",
    "# 편향 b도 0으로 초기화하고, 학습을 통해 값이 변경되는 변수임을 명시함.\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "888059e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.],\n",
      "        [0.],\n",
      "        [0.]], grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 4. 가설 세우기\n",
    "hypothesis = x_train * W + b\n",
    "print(hypothesis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "393ea675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18.6667, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 5. 비용 함수 선언하기\n",
    "# 선형 회귀의 비용 함수에 해당되는 평균 제곱 오차를 선언함\n",
    "cost = torch.mean((hypothesis - y_train)**2)\n",
    "print(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ad98d391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. 경사 하강법 구현하기\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "#gradient를 0으로 초기화\n",
    "optimizer.zero_grad()\n",
    "#비용 함수를 미분하여 gradient 계산\n",
    "cost.backward()\n",
    "#W와 b를 업데이트\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85380d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1999  W: 0.187,  b: 0.080  Cost: 18.666666\n",
      "Epoch  100/1999  W: 1.746,  b: 0.578  Cost: 0.048171\n",
      "Epoch  200/1999  W: 1.800,  b: 0.454  Cost: 0.029767\n",
      "Epoch  300/1999  W: 1.843,  b: 0.357  Cost: 0.018394\n",
      "Epoch  400/1999  W: 1.876,  b: 0.281  Cost: 0.011366\n",
      "Epoch  500/1999  W: 1.903,  b: 0.221  Cost: 0.007024\n",
      "Epoch  600/1999  W: 1.924,  b: 0.174  Cost: 0.004340\n",
      "Epoch  700/1999  W: 1.940,  b: 0.136  Cost: 0.002682\n",
      "Epoch  800/1999  W: 1.953,  b: 0.107  Cost: 0.001657\n",
      "Epoch  900/1999  W: 1.963,  b: 0.084  Cost: 0.001024\n",
      "Epoch 1000/1999  W: 1.971,  b: 0.066  Cost: 0.000633\n",
      "Epoch 1100/1999  W: 1.977,  b: 0.052  Cost: 0.000391\n",
      "Epoch 1200/1999  W: 1.982,  b: 0.041  Cost: 0.000242\n",
      "Epoch 1300/1999  W: 1.986,  b: 0.032  Cost: 0.000149\n",
      "Epoch 1400/1999  W: 1.989,  b: 0.025  Cost: 0.000092\n",
      "Epoch 1500/1999  W: 1.991,  b: 0.020  Cost: 0.000057\n",
      "Epoch 1600/1999  W: 1.993,  b: 0.016  Cost: 0.000035\n",
      "Epoch 1700/1999  W: 1.995,  b: 0.012  Cost: 0.000022\n",
      "Epoch 1800/1999  W: 1.996,  b: 0.010  Cost: 0.000013\n",
      "Epoch 1900/1999  W: 1.997,  b: 0.008  Cost: 0.000008\n"
     ]
    }
   ],
   "source": [
    "# 7. 전체 코드\n",
    "#데이터\n",
    "x_train = torch.FloatTensor([[1], [2], [3]])\n",
    "y_train = torch.FloatTensor([[2], [4], [6]])\n",
    "\n",
    "#모델 초기화\n",
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "\n",
    "nb_epochs = 1999 #원하는만큼 경사 하강법 반복\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    #H(x) 계산\n",
    "    hypothesis = x_train * W +b\n",
    "    \n",
    "    #cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train)**2)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{}  W: {:.3f},  b: {:.3f}  Cost: {:.6f}\".format(\n",
    "            epoch, nb_epochs, W.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "431f5df2",
   "metadata": {},
   "source": [
    "## 3-2. 자동 미분(Autograd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b85b289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "수식을 w로 미분한 값: 8.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "w = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "y = w**2\n",
    "z = 2*y + 5\n",
    "\n",
    "# 해당 수식을 w에 대해 미분\n",
    "z.backward()\n",
    "\n",
    "print('수식을 w로 미분한 값: {}'.format(w.grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458ac15d",
   "metadata": {},
   "source": [
    "## 3-3. 다중 선형 회귀(Multivariable Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "76ac0b9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/1000  w1: 0.294,  w2: 0.294,  w3: 0.297,  b: 0.003  Cost: 29661.800781\n",
      "Epoch  100/1000  w1: 0.674,  w2: 0.661,  w3: 0.676,  b: 0.008  Cost: 1.563628\n",
      "Epoch  200/1000  w1: 0.679,  w2: 0.655,  w3: 0.677,  b: 0.008  Cost: 1.497595\n",
      "Epoch  300/1000  w1: 0.684,  w2: 0.649,  w3: 0.677,  b: 0.008  Cost: 1.435044\n",
      "Epoch  400/1000  w1: 0.689,  w2: 0.643,  w3: 0.678,  b: 0.008  Cost: 1.375726\n",
      "Epoch  500/1000  w1: 0.694,  w2: 0.638,  w3: 0.678,  b: 0.009  Cost: 1.319507\n",
      "Epoch  600/1000  w1: 0.699,  w2: 0.633,  w3: 0.679,  b: 0.009  Cost: 1.266222\n",
      "Epoch  700/1000  w1: 0.704,  w2: 0.627,  w3: 0.679,  b: 0.009  Cost: 1.215703\n",
      "Epoch  800/1000  w1: 0.709,  w2: 0.622,  w3: 0.679,  b: 0.009  Cost: 1.167810\n",
      "Epoch  900/1000  w1: 0.713,  w2: 0.617,  w3: 0.680,  b: 0.009  Cost: 1.122429\n",
      "Epoch 1000/1000  w1: 0.718,  w2: 0.613,  w3: 0.680,  b: 0.009  Cost: 1.079390\n"
     ]
    }
   ],
   "source": [
    "#훈련 데이터 H(x) = w1x1 + w2x2 + w3x3 + b\n",
    "x1_train = torch.FloatTensor([[73], [93], [89], [96], [73]])\n",
    "x2_train = torch.FloatTensor([[80], [88], [91], [98], [66]])\n",
    "x3_train = torch.FloatTensor([[75], [93], [90], [100], [70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "#가중치 w와 편향 b 초기화\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "w3 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD([w1, w2, w3, b], lr=1e-5)\n",
    "\n",
    "nb_epochs = 1000\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    #H(x) 계산\n",
    "    hypothesis = x1_train*w1 + x2_train*w2 + x3_train*w3 + b\n",
    "    \n",
    "    #cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    #100번마다 로그 출력\n",
    "    if epoch % 100 == 0:\n",
    "        print(\"Epoch {:4d}/{}  w1: {:.3f},  w2: {:.3f},  w3: {:.3f},  b: {:.3f}  Cost: {:.6f}\".format(\n",
    "            epoch, nb_epochs, w1.item(), w2.item(), w3.item(), b.item(), cost.item()\n",
    "        ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "695ae2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    1/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    2/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    3/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    4/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    5/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    6/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    7/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    8/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch    9/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   10/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   11/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   12/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   13/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   14/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   15/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   16/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   17/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   18/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   19/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n",
      "Epoch   20/20  hypothesis: tensor([2.0066, 4.0040, 6.0013])  Cost: 28311.136719\n"
     ]
    }
   ],
   "source": [
    "#행렬 연산을 고려하여 파이토치로 구현하기\n",
    "x1_train = torch.FloatTensor([[73, 80, 75],\n",
    "                              [93, 88, 93],\n",
    "                              [89, 91, 80],\n",
    "                              [96, 98, 100],\n",
    "                              [73, 66, 70]])\n",
    "y_train = torch.FloatTensor([[152], [185], [180], [196], [142]])\n",
    "\n",
    "#가중치 w와 편향 b 초기화\n",
    "W2 = torch.zeros((3, 1), requires_grad=True)\n",
    "b2 = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "#optimizer 설정\n",
    "optimizer = optim.SGD([W2, b2], lr=1e-5)\n",
    "\n",
    "nb_epochs = 20\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    #H(x) 계산\n",
    "    hypothesis = x_train.matmul(W) + b\n",
    "    \n",
    "    #cost 계산\n",
    "    cost = torch.mean((hypothesis - y_train) ** 2)\n",
    "    \n",
    "    #cost로 H(x) 개선\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    print(\"Epoch {:4d}/{}  hypothesis: {}  Cost: {:.6f}\".format(\n",
    "        epoch, nb_epochs, hypothesis.squeeze().detach(), cost.item()\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05f3ec2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
